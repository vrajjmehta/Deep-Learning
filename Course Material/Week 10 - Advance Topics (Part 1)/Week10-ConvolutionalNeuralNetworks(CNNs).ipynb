{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week11-ConvolutionalNeuralNetworks(CNNs).ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0FslCE7TeuuQ","colab_type":"text"},"source":["# Convolutional Neural Networks (CNNs)\n","\n","### Welcome to the 10th Lab of 42028: Deep Learning and CNN!\n","\n","In this  Lab/Tutorial session you will be implementing Inception module and create a Convolutional Neural Network for image classification .\n","\n","So lets get started!\n","\n","## Tutorial:\n","Implementation of Inception based Micro-GoogleNet CNN architecture using Keras for classfication of Cifar-10 dataset.\n","\n","## Tasks for this week:\n","\n","1. Implementation of Inception based Mini-GoogleNet CNN architecture for Cifar-10 classification using Keras API. \n","2. Train and test model\n","\n","\n","**Reference and adapted from:** [https://www.analyticsvidhya.com/blog/2018/10/understanding-inception-network-from-scratch/](https://www.analyticsvidhya.com/blog/2018/10/understanding-inception-network-from-scratch/)"]},{"cell_type":"markdown","metadata":{"id":"3WpnnqodguDh","colab_type":"text"},"source":["## The Inception Begins !"]},{"cell_type":"markdown","metadata":{"id":"rahZjYapfk0y","colab_type":"text"},"source":["### Step 1: Import required packages\n","\n","we will need tensorflow, numpy, os and keras\n"]},{"cell_type":"code","metadata":{"id":"OB0ONDu2OVEm","colab_type":"code","colab":{}},"source":["import keras\n","from keras.layers.core import Layer\n","import keras.backend as K\n","import tensorflow as tf\n","\n","from keras.models import Model\n","from keras.layers import Conv2D, MaxPool2D,  \\\n","    Dropout, Dense, Input, concatenate,      \\\n","    GlobalAveragePooling2D, AveragePooling2D,\\\n","    Flatten\n","\n","import cv2 \n","import numpy as np \n","from keras.datasets import cifar10 \n","from keras import backend as K \n","from keras.utils import np_utils\n","\n","import math \n","from keras.optimizers import SGD \n","from keras.callbacks import LearningRateScheduler"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3rWEMlkihytK","colab_type":"text"},"source":["### Step-2: Design the Inception V1 module\n","\n","<img src='http://drive.google.com/uc?export=view&id=12vu9O4-f-jeuQIWg6nwAUFiZV8qwKM9j' alt='Conv'>\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"mx-iv1fypAlS","colab_type":"code","colab":{}},"source":["# Create the Inception module\n","def inception_module(x,\n","                     filters_1x1,\n","                     filters_3x3_reduce,\n","                     filters_3x3,\n","                     filters_5x5_reduce,\n","                     filters_5x5,\n","                     filters_pool_proj,\n","                     name=None):\n","    \n","    # 1X1 CONV\n","    conv_1x1 = Conv2D(filters_1x1, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n","    \n","    # 1X1 CONV --> 3x3 CONV\n","    conv_3x3 = Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n","    conv_3x3 = Conv2D(filters_3x3, (3, 3), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_3x3)\n","\n","    # 1X1 CONV --> 5x5 CONV\n","    conv_5x5 = Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n","    conv_5x5 = Conv2D(filters_5x5, (5, 5), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_5x5)\n","\n","    # 3X3 MAXPOOL --> 1X1 CONV\n","    pool_proj = MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)\n","    pool_proj = Conv2D(filters_pool_proj, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(pool_proj)\n","\n","    # Concatenate the layers\n","    output = concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=3, name=name)\n","    \n","    return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yb5LCr9OsOM_","colab_type":"code","colab":{}},"source":["kernel_init = keras.initializers.glorot_uniform()\n","bias_init = keras.initializers.Constant(value=0.2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tEmXUpDmh-Z1","colab_type":"text"},"source":["### Step 3: Design a Micro-GoogleNet with 1 Layer of Inception V1\n","\n","<img src='http://drive.google.com/uc?export=view&id=143ci2dp1TKZb6o9fXXR3122ewmXcxreL' alt='Conv'>\n","\n"]},{"cell_type":"code","metadata":{"id":"LJiHGAsSsepM","colab_type":"code","colab":{}},"source":["# Create the input layer\n","input_layer = Input(shape=(32, 32, 3)) # Use the actual input size\n","\n","# CONV Layer\n","x = Conv2D(64, (3, 3), padding='same', strides=(1, 1), activation='relu', name='conv_1_3x3/2', kernel_initializer=kernel_init, bias_initializer=bias_init)(input_layer)\n","x = MaxPool2D((3, 3), padding='same', strides=(1, 1), name='max_pool_1_3x3/2')(x)\n","\n","#Adding Inception layer to te modeel\n","x = inception_module(x,\n","                     filters_1x1=64,\n","                     filters_3x3_reduce=96,\n","                     filters_3x3=128,\n","                     filters_5x5_reduce=16,\n","                     filters_5x5=32,\n","                     filters_pool_proj=32,\n","                     name='inception_3a')\n","\n","x = GlobalAveragePooling2D(name='avg_pool_5_3x3/1')(x)\n","\n","x = Dropout(0.4)(x)\n","\n","x = Dense(10, activation='softmax', name='output')(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WoYM0CJQshM6","colab_type":"code","colab":{}},"source":["# Form the model\n","model = Model(input_layer, x, name='inception_v1')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3jYRiIKEso2s","colab_type":"code","colab":{}},"source":["#Display Model Summary\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g1gJI8OsOuZQ","colab_type":"text"},"source":["### Step 4: Download the Cifar-10 dataset using keras"]},{"cell_type":"code","metadata":{"id":"ATljwCk0BdmE","colab_type":"code","colab":{}},"source":["num_classes = 10 #Number of classes in the dataset\n","\n","def load_cifar10_data(img_rows, img_cols):\n","\n","    # Load cifar10 training and validation sets\n","    (X_train, Y_train), (X_valid, Y_valid) = cifar10.load_data()\n","    \n","    # Resize training images\n","    X_train = np.array([cv2.resize(img, (img_rows,img_cols)) for img in X_train[:,:,:,:]])\n","    X_valid = np.array([cv2.resize(img, (img_rows,img_cols)) for img in X_valid[:,:,:,:]])\n","\n","    # Transform targets to keras compatible format\n","    Y_train = np_utils.to_categorical(Y_train, num_classes)\n","    Y_valid = np_utils.to_categorical(Y_valid, num_classes)\n","    \n","    X_train = X_train.astype('float32')\n","    X_valid = X_valid.astype('float32')\n","\n","    # preprocess data\n","    X_train = X_train / 255.0\n","    X_valid = X_valid / 255.0\n","\n","    return X_train, Y_train, X_valid, Y_valid"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hNZ63cL7PuzM","colab_type":"code","colab":{}},"source":["# Display the shapes of the training images\n","X_train, y_train, X_test, y_test = load_cifar10_data(32, 32)\n","print(X_train.shape)\n","print(X_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Qh2YwnckTbm","colab_type":"code","colab":{}},"source":["## Display an image from the dataset\n","import matplotlib.pyplot as plt\n","plt.imshow(X_train[300])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ryE__lGFi91_","colab_type":"text"},"source":["### Step 5: Define the Hyper-parameters, Compile the model, Start training"]},{"cell_type":"code","metadata":{"id":"uClLG7U6ALDK","colab_type":"code","colab":{}},"source":["#Define the number of epochs and learning rate\n","epochs = 25\n","initial_lrate = 0.01\n","\n","#Create Learning rate decay\n","def decay(epoch, steps=100):\n","    initial_lrate = 0.01\n","    drop = 0.96\n","    epochs_drop = 8\n","    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n","    return lrate\n","\n","#Define SGD paramenters\n","sgd = SGD(lr=initial_lrate, momentum=0.9, nesterov=False)\n","\n","lr_sc = LearningRateScheduler(decay, verbose=1)\n","\n","## Compile Model\n","model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CzqrmvfVAieb","colab_type":"code","colab":{}},"source":["#Start training the model\n","history1 = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=256, callbacks=[lr_sc])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fdpkn4XVewUk","colab_type":"code","colab":{}},"source":["## Plot the Traning and Validation loss\n","loss = history1.history['loss']\n","val_loss = history1.history['val_loss']\n","\n","epochs = range(len(loss))\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'r', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q1zFbQggpTfP","colab_type":"text"},"source":["## Your task begins now!"]},{"cell_type":"markdown","metadata":{"id":"okL5nlyehYRv","colab_type":"text"},"source":["## Task 1: Design a Mini-GoogleNet with Inception V1 model\n","\n","### Add four inception layers in the Micro-GoogleNet architecture created above in the following order and configuration:\n","\n","<img src='http://drive.google.com/uc?export=view&id=1674k5lS1gTnT7-YaqhoKEDQadttsJMcl' alt='Conv'>\n"]},{"cell_type":"code","metadata":{"id":"3RajJeyNUbqS","colab_type":"code","colab":{}},"source":["## WRITE YOUR CODE HERE ## \n","# Define the input layer\n","input_layer = \n","\n","# Add the first CONV layer and MaxPOOL layer\n","x = \n","x = \n","\n","## Add Inception layer-1\n","x = \n","\n","## Add Inception layer-2\n","x = \n","\n","## Add MaxPool\n","x = \n","\n","## Add Inception layer-3\n","x = \n","\n","## Add Inception layer-4\n","x = \n","\n","## Add Avg Pool\n","x = \n","\n","## Add Dropout\n","x = \n","\n","## Add Dense layer\n","x = \n","\n","## END YOUR CODE HERE ## "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tjSbOZc4U12S","colab_type":"code","colab":{}},"source":["## WRITE YOUR CODE HERE ## \n","# Form the Model\n","model = \n","\n","## END YOUR CODE HERE ##"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rVXKF4IAU88Q","colab_type":"code","colab":{}},"source":["# Print Model Summary\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nFJqCJSNAFHn","colab_type":"text"},"source":["## Task 2: Define the HyperParamenters, Optimzer, etc"]},{"cell_type":"code","metadata":{"id":"GzOOJxf9_o18","colab_type":"code","colab":{}},"source":["## WRITE YOUR CODE HERE ## \n","#Define the number of epochs and learning rate\n","epochs =\n","initial_lrate = \n","## END YOUR CODE HERE ##\n","\n","#Create Learning rate decay\n","def decay(epoch, steps=100):\n","    initial_lrate = 0.01\n","    drop = 0.96\n","    epochs_drop = 8\n","    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n","    return lrate\n","\n","## WRITE YOUR CODE HERE ##\n","#Define SGD paramenters\n","sgd = \n","\n","lr_sc = \n","## END YOUR CODE HERE ##"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q5njAq5rAbhV","colab_type":"text"},"source":["## Task 3: Compile and Train the Model"]},{"cell_type":"code","metadata":{"id":"USiokHx3VLJB","colab_type":"code","colab":{}},"source":["# Compile the model and start training\n","## WRITE YOUR CODE HERE ##\n","model.compile() ## Add the missing code\n","history = model.fit() ## Add the missing code\n","## END YOUR CODE HERE ##"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0pD3HSb4AjPr","colab_type":"text"},"source":["## Task 4: Test on Train and Test set"]},{"cell_type":"code","metadata":{"id":"PeL8Re-YVBhA","colab_type":"code","colab":{}},"source":["# Test the model on Test and Validation dataset\n","## WRITE YOUR CODE HERE ##\n","model.evaluate() ## Add the missing code for Training dataset\n","model.evaluate() ## Add the missing code for Test dataset\n","## END YOUR CODE HERE ##"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1_yTgda6AqDo","colab_type":"text"},"source":["## Task 5: Display the Train and Validation Loss curve"]},{"cell_type":"code","metadata":{"id":"HKejCK_AeaEC","colab_type":"code","colab":{}},"source":["# Display the train and validation loss\n","## WRITE YOUR CODE HERE ##\n","loss = \n","val_loss = \n","\n","epochs = \n","\n","## END YOUR CODE HERE ##\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'r', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n"],"execution_count":0,"outputs":[]}]}